# 2022.07.01

import os
import argparse
import time
import numpy as np
import torch
import utils
import torch.nn as nn 
import torch.optim as optim
from torchvision import datasets, transforms
import random
import utils
import vgg16
import transform_net

IMAGENET_MEAN_255 = [103.53, 116.28, 123.675]
IMAGENET_STD_NEUTRAL = [1, 1, 1]

os.makedirs ('./weights', exist_ok = True)
os.makedirs ('./transform_weight', exist_ok = True)

config = argparse.ArgumentParser (description = 'Style Trasfer')
config.add_argument ('--epochs', type = int, default = 1, help = 'Number of iterations for training the transformer model')
config.add_argument ('--gpu_id', type = str, default = '0', help = 'GPU No. if multiplt gpus have been used else 0')
config.add_argument ('--img_size', type = int, default = 256, help = 'Image size on the training stage')
config.add_argument ('--seed', type = int, default = 1, help = 'Random Seed')
config.add_argument ('--dataset_dir', type = str, default = './images/content_images', help = 'Path for datasets for training')
config.add_argument ('--batch_size', type = int, default = 4, help = 'The batch size for training')
config.add_argument ('--learning_rate', type = float, default = 1e-3, help = 'Learning rate for the optimizer')
config.add_argument ('--style_img_path', type = str, default = './images/style_images/mosaic.jpg', help = 'The style image')
config.add_argument ('--content_weight', type = float, default = 5, help = 'The weight of content loss when compting the total loss')
config.add_argument ('--style_weight', type = float, default = 5, help = 'The weight of style loss when compting the total loss')
config.add_argument ('--interval', type = int, default = 2, help = 'Save and show the model after every interval times')
config.add_argument ('--save_model_path', type = str, default = './weights/', help = 'Path for the model saved')
config.add_argument ('--save_img_path', type = str, default = './images/results/', help = 'Path for saving the sample images and result images')
config.add_argument ('--transform_model_path', type = str, default = 'transform', help = 'Path for the saving the transform model')

def train (args):
    # Device setting
    device = ("cuda" if torch.cuda.is_available() else "cpu")

    # Multi-threads setting
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_id

    # Fix the random seed
    utils.fixRandomSeed(args.seed)

    # Generate the dataset and the dataloader
    transform = utils.transformImage (args.img_size)
    data_loader = utils.loadData (args.dataset_dir, transform, args.batch_size)

    # Define Neural Networks
    VGG = vgg16.vgg16_pretrained().to(device)
    TransformNet = transform_net.TransformNet().to(device)

    # Define the optimizer
    optimizer = optim.Adam (TransformNet.parameters(), lr = args.learning_rate)

    # Define loss function
    loss = nn.MSELoss().to(device)

    # Obtain style representations/features
    style_features = utils.getStyleFeatures (device, args.style_img_path, args.batch_size, VGG)

    # Initalizing Loss Histories
    content_loss_history = []
    style_loss_history = []
    total_loss_history = []
    accumulated_content_loss = 0
    accumulated_style_loss = 0
    accumulated_total_loss = 0

    # Optimization/Training Loop
    num_step = 0
    start_time = time.time()

    # Training
    for epoch in range (50):
        for fold, _ in data_loader:
            num_step += 1
            batch_size = fold.shape[0]

            # Free the unimportant tensors to release the GPU memory
            torch.cuda.empty_cache()

            # Zero_out Gradients
            optimizer.zero_grad()

            # Compute the transformed batch/fold formed by the transform net
            fold = fold[:, [2, 1, 0]].to (device)
            transformed_fold = TransformNet (fold)
            fold_n = transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)(fold)
            transformed_fold_n = transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)(transformed_fold)

            # Compute the features generated by the pretrained VGG networks and the transform network
            content_features = VGG (fold_n)
            transformed_features = VGG (transformed_fold_n)

            # Compute the losses
            content_loss, style_loss, total_loss = utils.computeTrainingLoss (args.content_weight, args.style_weight, content_features, transformed_features, style_features, batch_size, loss)

            # Record the training history & accumulated loss
            accumulated_content_loss += content_loss
            accumulated_style_loss += style_loss
            accumulated_total_loss += total_loss

            # Back propagation
            total_loss.backward()
            optimizer.step()

            # Show the training history
            if (num_step + 1) % args.interval == 0:
                # Print the loss 
                utils.printLosses (num_step, args.epochs, len(data_loader), accumulated_content_loss, accumulated_style_loss, accumulated_total_loss, start_time)

                # Save the model
                utils.saveModel (args.save_model_path, num_step, TransformNet)

                # Save the sample image from the training stage.
                utils.saveTrainingSampleImage (transformed_fold, args.save_img_path, num_step)  

                # Record the loss history
                content_loss_history.append(accumulated_total_loss / num_step)
                style_loss_history.append(accumulated_style_loss / num_step)
                total_loss_history.append(accumulated_total_loss / num_step)

                # Clear the accumulated loss 
                accumulated_content_loss = 0
                accumulated_style_loss = 0
                accumulated_total_loss = 0
    stop_time = time.time()

    # Save the summary of the whole training process
    utils.printSummary ("Transform", start_time, stop_time, content_loss_history, style_loss_history, total_loss_history)

    # Save the losses function 
    utils.saveLossAsCsv (content_loss_history, style_loss_history, total_loss_history)
    
    # Save the TransformerNetwork model
    utils.saveTransformModel (TransformNet, args.transform_model_path)

if __name__ == "__main__":
    args = config.parse_args()
    train (args)